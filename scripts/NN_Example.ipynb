{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from neuralop.models import FNO\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from neuralop.models import TFNO\n",
    "from neuralop import Trainer\n",
    "from neuralop.datasets import load_darcy_flow_small\n",
    "from neuralop.utils import count_params\n",
    "from neuralop import LpLoss, H1Loss\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from neuralop.utils import UnitGaussianNormalizer\n",
    "from neuralop.datasets.tensor_dataset import TensorDataset\n",
    "from neuralop.datasets.transforms import PositionalEmbedding\n",
    "\n",
    "\n",
    "def load_dataset(train_path, test_path,\n",
    "                batch_size, test_batch_sizes,\n",
    "                grid_boundaries=[[0,1],[0,1]],\n",
    "                positional_encoding=False,\n",
    "                encode_input=False,\n",
    "                encode_output=False,\n",
    "                encoding='pixel-wise', \n",
    "                channel_dim=1):\n",
    "    \"\"\"Loads a small Darcy-Flow dataset\n",
    "    \n",
    "    Training contains 1000 samples in resolution 16x16. \n",
    "    Testing contains 100 samples at resolution 16x16 and\n",
    "    50 samples at resolution 32x32.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_train : int\n",
    "    n_tests : int\n",
    "    batch_size : int\n",
    "    test_batch_sizes : int list\n",
    "    test_resolutions : int list, default is [16, 32],\n",
    "    grid_boundaries : int list, default is [[0,1],[0,1]],\n",
    "    positional_encoding : bool, default is True\n",
    "    encode_input : bool, default is False\n",
    "    encode_output : bool, default is True\n",
    "    encoding : 'channel-wise'\n",
    "    channel_dim : int, default is 1\n",
    "        where to put the channel dimension, defaults size is batch, channel, height, width\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    training_dataloader, testing_dataloaders\n",
    "\n",
    "    training_dataloader : torch DataLoader\n",
    "    testing_dataloaders : dict (key: DataLoader)\n",
    "    \"\"\"\n",
    "    # for res in test_resolutions:\n",
    "    #     if res not in [16, 32]:\n",
    "    #         raise ValueError(f'Only 32 and 64 are supported for test resolution, but got {test_resolutions=}')\n",
    "    # path = Path(__file__).resolve().parent.joinpath('data')\n",
    "    return load_dataset_pt(train_path, test_path, \n",
    "                         batch_size=batch_size, test_batch_size=test_batch_sizes,\n",
    "                         grid_boundaries=grid_boundaries,\n",
    "                         positional_encoding=positional_encoding,\n",
    "                         encode_input=encode_input,\n",
    "                         encode_output=encode_output,\n",
    "                         encoding=encoding,\n",
    "                         channel_dim=channel_dim)\n",
    "\n",
    "def load_dataset_pt(train_data_path, test_data_path,\n",
    "                batch_size, test_batch_size,\n",
    "                grid_boundaries=[[0,1],[0,1]],\n",
    "                positional_encoding=False,\n",
    "                encode_input=False,\n",
    "                encode_output=True,\n",
    "                encoding='channel-wise', \n",
    "                channel_dim=1):\n",
    "    \"\"\"Load the Navier-Stokes dataset\n",
    "    \"\"\"\n",
    "    data = np.load(train_data_path)\n",
    "    # n_train = data.shape[0]\n",
    "    x_train = data[:, 0]\n",
    "    y_train = data[:, 1]\n",
    "    del data\n",
    "\n",
    "    data = np.load(test_data_path)\n",
    "    # n_test = data.shape[0]\n",
    "    x_test = data[:, 0]\n",
    "    y_test = data[:, 1]\n",
    "    del data\n",
    "    \n",
    "    # if encode_input:\n",
    "    #     if encoding == 'channel-wise':\n",
    "    #         reduce_dims = list(range(x_train.ndim))\n",
    "    #     elif encoding == 'pixel-wise':\n",
    "    #         reduce_dims = [0]\n",
    "\n",
    "    #     input_encoder = UnitGaussianNormalizer(x_train, reduce_dim=reduce_dims)\n",
    "    #     x_train = input_encoder.encode(x_train)\n",
    "    #     x_test = input_encoder.encode(x_test.contiguous())\n",
    "    # else:\n",
    "    #     input_encoder = None\n",
    "\n",
    "    # if encode_output:\n",
    "    #     if encoding == 'channel-wise':\n",
    "    #         reduce_dims = list(range(y_train.ndim))\n",
    "    #     elif encoding == 'pixel-wise':\n",
    "    #         reduce_dims = [0]\n",
    "\n",
    "    #     output_encoder = UnitGaussianNormalizer(y_train, reduce_dim=reduce_dims)\n",
    "    #     y_train = output_encoder.encode(y_train)\n",
    "    # else:\n",
    "    #     output_encoder = None\n",
    "    output_encoder = None\n",
    "\n",
    "    train_db = TensorDataset(x_train, y_train, transform_x=PositionalEmbedding(grid_boundaries, 0) if positional_encoding else None)\n",
    "    train_loader = torch.utils.data.DataLoader(train_db,\n",
    "                                            batch_size=batch_size, shuffle=True,\n",
    "                                            num_workers=0, pin_memory=True, persistent_workers=False)\n",
    "\n",
    "    test_db = TensorDataset(x_test, y_test,transform_x=PositionalEmbedding(grid_boundaries, 0) if positional_encoding else None)\n",
    "    test_loader = torch.utils.data.DataLoader(test_db,\n",
    "                                              batch_size=test_batch_size, shuffle=False,\n",
    "                                              num_workers=0, pin_memory=True, persistent_workers=False)\n",
    "    test_loaders =  {'1': test_loader}\n",
    "    # for ( n_test, test_batch_size) in zip(n_test, test_batch_sizes):\n",
    "    #     print(f'Loading test db with {n_test} samples and batch-size={test_batch_size}')\n",
    "    #     data = torch.load(Path(data_path).joinpath(f'darcy_test_{res}.pt').as_posix())\n",
    "    #     x_test = data['x'][:n_test, :, :].unsqueeze(channel_dim).type(torch.float32).clone()\n",
    "    #     y_test = data['y'][:n_test, :, :].unsqueeze(channel_dim).clone()\n",
    "    #     del data \n",
    "    #     if input_encoder is not None:\n",
    "    #         x_test = input_encoder.encode(x_test)\n",
    "\n",
    "    #     test_db = TensorDataset(x_test, y_test, transform_x=PositionalEmbedding(grid_boundaries, 0) if positional_encoding else None)\n",
    "    #     test_loader = torch.utils.data.DataLoader(test_db,\n",
    "    #                                               batch_size=test_batch_size, shuffle=False,\n",
    "    #                                               num_workers=0, pin_memory=True, persistent_workers=False)\n",
    "    #     test_loaders[res] = test_loader\n",
    "\n",
    "    return train_loader, test_loaders, output_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/input_train.npy'\n",
    "train_loader, test_loaders, output_encoder = load_dataset(,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our model has 4228097 parameters.\n"
     ]
    }
   ],
   "source": [
    "model = FNO(n_modes=(16, 16), hidden_channels=64,\n",
    "                in_channels=3, out_channels=1)\n",
    "model = model.to(device)\n",
    "\n",
    "n_params = count_params(model)\n",
    "print(f'\\nOur model has {n_params} parameters.')\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### MODEL ###\n",
      " FNO(\n",
      "  (fno_blocks): FNOBlocks(\n",
      "    (convs): FactorizedSpectralConv(\n",
      "      (weight): ModuleList(\n",
      "        (0-7): 8 x ComplexDenseTensor(shape=torch.Size([64, 64, 8, 8]), rank=None)\n",
      "      )\n",
      "    )\n",
      "    (fno_skips): ModuleList(\n",
      "      (0-3): 4 x Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (lifting): Lifting(\n",
      "    (fc): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (projection): Projection(\n",
      "    (fc1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fc2): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "\n",
      "### OPTIMIZER ###\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.008\n",
      "    lr: 0.008\n",
      "    maximize: False\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "\n",
      "### SCHEDULER ###\n",
      " <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x000001D45EA6E500>\n",
      "\n",
      "### LOSSES ###\n",
      "\n",
      " * Train: <neuralop.training.losses.H1Loss object at 0x000001D468F573A0>\n",
      "\n",
      " * Test: {'h1': <neuralop.training.losses.H1Loss object at 0x000001D468F573A0>, 'l2': <neuralop.training.losses.LpLoss object at 0x000001D45EA6E440>}\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                lr=8e-3, \n",
    "                                weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "\n",
    "l2loss = LpLoss(d=1, p=2)\n",
    "h1loss = H1Loss(d=1)\n",
    "\n",
    "train_loss = h1loss\n",
    "eval_losses={'h1': h1loss, 'l2': l2loss}\n",
    "\n",
    "print('\\n### MODEL ###\\n', model)\n",
    "print('\\n### OPTIMIZER ###\\n', optimizer)\n",
    "print('\\n### SCHEDULER ###\\n', scheduler)\n",
    "print('\\n### LOSSES ###')\n",
    "print(f'\\n * Train: {train_loss}')\n",
    "print(f'\\n * Test: {eval_losses}')\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on regular inputs (no multi-grid patching).\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 12\u001b[0m\n\u001b[0;32m      2\u001b[0m input_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39m../data/input_test.npy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(model, n_epochs\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m,\n\u001b[0;32m      5\u001b[0m                   device\u001b[39m=\u001b[39mdevice,\n\u001b[0;32m      6\u001b[0m                   mg_patching_levels\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m                   use_distributed\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     10\u001b[0m                   verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 12\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(input_train, input_test,\n\u001b[0;32m     13\u001b[0m               \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     14\u001b[0m               model, \n\u001b[0;32m     15\u001b[0m               optimizer,\n\u001b[0;32m     16\u001b[0m               scheduler, \n\u001b[0;32m     17\u001b[0m               regularizer\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \n\u001b[0;32m     18\u001b[0m               training_loss\u001b[39m=\u001b[39;49mtrain_loss,\n\u001b[0;32m     19\u001b[0m               eval_losses\u001b[39m=\u001b[39;49meval_losses)\n",
      "File \u001b[1;32mc:\\Users\\adria\\miniforge3\\envs\\my-env\\lib\\site-packages\\neuralop\\training\\trainer.py:71\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, train_loader, test_loaders, output_encoder, model, optimizer, scheduler, regularizer, training_loss, eval_losses)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m, train_loader, test_loaders, output_encoder,\n\u001b[0;32m     68\u001b[0m           model, optimizer, scheduler, regularizer, \n\u001b[0;32m     69\u001b[0m           training_loss\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, eval_losses\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     70\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Trains the given model on the given datasets\"\"\"\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m     n_train \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_loader\u001b[39m.\u001b[39;49mdataset)\n\u001b[0;32m     73\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(test_loaders, \u001b[39mdict\u001b[39m):\n\u001b[0;32m     74\u001b[0m         test_loaders \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(test\u001b[39m=\u001b[39mtest_loaders)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'dataset'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "trainer = Trainer(model, n_epochs=20,\n",
    "                  device=device,\n",
    "                  mg_patching_levels=0,\n",
    "                  wandb_log=False,\n",
    "                  log_test_interval=3,\n",
    "                  use_distributed=False,\n",
    "                  verbose=True)\n",
    "\n",
    "trainer.train(input_train, input_test,\n",
    "              None,\n",
    "              model, \n",
    "              optimizer,\n",
    "              scheduler, \n",
    "              regularizer=False, \n",
    "              training_loss=train_loss,\n",
    "              eval_losses=eval_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
